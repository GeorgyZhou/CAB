{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import DataSet\n",
    "from tensorflow.contrib.learn.python.learn.datasets import base\n",
    "from tensorflow.python.framework import dtypes\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from IPython import display\n",
    "from copy import deepcopy\n",
    "%matplotlib inline\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import logic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Activation function and its derivative\n",
    "def sigma(x):\n",
    "    return tf.div(tf.constant(1.0),\n",
    "                  tf.add(tf.constant(1.0), tf.exp(tf.negative(x))))\n",
    "\n",
    "def sigmaprime(x):\n",
    "    return tf.multiply(sigma(x), tf.subtract(tf.constant(1.0), sigma(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "#Create 10 permuted MNIST datasets\n",
    "def permute_mnist(mnist):\n",
    "    perm_inds = range(mnist.train.images.shape[1])\n",
    "    np.random.shuffle(perm_inds)\n",
    "    mnist2 = deepcopy(mnist)\n",
    "    sets = [\"train\", \"validation\", \"test\"]\n",
    "    for set_name in sets:\n",
    "        this_set = getattr(mnist2, set_name) # shallow copy\n",
    "        this_set._images = np.transpose(np.array([this_set.images[:,c] for c in perm_inds]))\n",
    "    return mnist2\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "mnist_list = [mnist]\n",
    "for j in xrange(9):\n",
    "    mnist_list.append(permute_mnist(mnist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Define a 2 layer feedfoward network with 100 hidden neurons \n",
    "a_0 = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "middle = 100\n",
    "\n",
    "w_1 = tf.Variable(tf.truncated_normal([785, middle], stddev=0.1))\n",
    "w_2 = tf.Variable(tf.truncated_normal([middle+1, 10], stddev=0.1))\n",
    "\n",
    "w_old_1 = tf.Variable(tf.zeros([785, middle]))\n",
    "w_old_2 = tf.Variable(tf.zeros([middle+1, 10]))\n",
    "\n",
    "\n",
    "\n",
    "#Conceptors for used spaces\n",
    "A_0 = np.zeros([785, 785])\n",
    "A_1 = np.zeros([middle+1, middle+1])\n",
    "\n",
    "#Conceptors for free spaces\n",
    "F_0 = tf.Variable(tf.eye(785))\n",
    "F_1 = tf.Variable(tf.eye(middle+1))\n",
    "\n",
    "\n",
    "#Forward Pass, ab_i is the state vector together with bias\n",
    "ab_0 = tf.concat([a_0, tf.tile(tf.ones([1,1]), [tf.shape(a_0)[0], 1])], 1)\n",
    "z_1 = tf.matmul(ab_0, w_1)\n",
    "a_1 = sigma(z_1)\n",
    "ab_1 = tf.concat([a_1, tf.tile(tf.ones([1,1]), [tf.shape(a_1)[0], 1])], 1)\n",
    "z_2 = tf.matmul(ab_1, w_2)\n",
    "a_2 = sigma(z_2)\n",
    "\n",
    "diff = tf.subtract(a_2, y)\n",
    "\n",
    "\n",
    "#Backward Pass\n",
    "reg2 = tf.Variable(0.001)\n",
    "reg1 = tf.Variable(0.001)\n",
    "\n",
    "d_z_2 = tf.multiply(diff, sigmaprime(z_2))\n",
    "d_w_2 = tf.matmul(tf.transpose(tf.matmul(ab_1,F_1)), d_z_2)\n",
    "\n",
    "inc_w_2 = tf.subtract(w_2, w_old_2)\n",
    "reg_w_2 = tf.multiply(reg2, inc_w_2)\n",
    "d_w_2 = tf.add(d_w_2, reg_w_2)\n",
    "\n",
    "\n",
    "d_ab_1 = tf.matmul(d_z_2, tf.transpose(w_2))\n",
    "d_a_1 = d_ab_1[:, :-1]\n",
    "d_z_1 = tf.multiply(d_a_1, sigmaprime(z_1))\n",
    "d_w_1 = tf.matmul(tf.transpose(tf.matmul(ab_0,F_0)), d_z_1)\n",
    "\n",
    "inc_w_1 = tf.subtract(w_1, w_old_1)\n",
    "reg_w_1 = tf.multiply(reg1, inc_w_1)\n",
    "d_w_1 = tf.add(d_w_1, reg_w_1)\n",
    "\n",
    "eta = tf.constant(0.1)\n",
    "step = [\n",
    "    tf.assign(w_1,\n",
    "            tf.subtract(w_1, tf.multiply(eta, d_w_1)))\n",
    "\n",
    "  , tf.assign(w_2,\n",
    "            tf.subtract(w_2, tf.multiply(eta, d_w_2)))\n",
    "]\n",
    "\n",
    "#Compute Classification Accuracy\n",
    "acct_mat = tf.equal(tf.argmax(a_2, 1), tf.argmax(y, 1))\n",
    "acct_res = tf.reduce_sum(tf.cast(acct_mat, tf.float32))\n",
    "\n",
    "#Update the old weights, which are the weights before training a task\n",
    "updateW_old = [tf.assign(w_old_1, w_1), tf.assign(w_old_2, w_2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initialize variables\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MNIST 1\n",
      "Validation accuracy: 0.1\n",
      "Validation accuracy: 0.56\n",
      "Validation accuracy: 0.856\n",
      "Validation accuracy: 0.954\n",
      "Validation accuracy: 0.955\n",
      "Validation accuracy: 0.961\n",
      "Validation accuracy: 0.959\n",
      "Validation accuracy: 0.968\n",
      "Validation accuracy: 0.967\n",
      "Validation accuracy: 0.964\n",
      "w_1 norm 30.829035\n",
      "w_2 norm 25.061602\n",
      "Accuracy on Current Dataset 0.9707\n",
      "Test on all Previous Datasets:\n",
      "0.9707\n",
      "Current Average Accuracy: 0.9707\n",
      "Training MNIST 2\n",
      "Validation accuracy: 0.081\n",
      "Validation accuracy: 0.946\n",
      "Validation accuracy: 0.959\n",
      "Validation accuracy: 0.964\n",
      "Validation accuracy: 0.968\n",
      "Validation accuracy: 0.972\n",
      "Validation accuracy: 0.973\n",
      "Validation accuracy: 0.974\n",
      "Validation accuracy: 0.97\n",
      "Validation accuracy: 0.973\n",
      "w_1 norm 41.850765\n",
      "w_2 norm 31.380337\n",
      "Accuracy on Current Dataset 0.9739\n",
      "Test on all Previous Datasets:\n",
      "0.9699\n",
      "0.9739\n",
      "Current Average Accuracy: 0.9719\n",
      "Training MNIST 3\n",
      "Validation accuracy: 0.13\n",
      "Validation accuracy: 0.951\n",
      "Validation accuracy: 0.958\n",
      "Validation accuracy: 0.965\n",
      "Validation accuracy: 0.966\n",
      "Validation accuracy: 0.965\n",
      "Validation accuracy: 0.97\n",
      "Validation accuracy: 0.972\n",
      "Validation accuracy: 0.968\n",
      "Validation accuracy: 0.97\n",
      "w_1 norm 49.61749\n",
      "w_2 norm 35.17357\n",
      "Accuracy on Current Dataset 0.9743\n",
      "Test on all Previous Datasets:\n",
      "0.9666\n",
      "0.9719\n",
      "0.9743\n",
      "Current Average Accuracy: 0.9709333333333333\n",
      "Training MNIST 4\n",
      "Validation accuracy: 0.13\n",
      "Validation accuracy: 0.947\n",
      "Validation accuracy: 0.962\n",
      "Validation accuracy: 0.962\n",
      "Validation accuracy: 0.954\n",
      "Validation accuracy: 0.964\n",
      "Validation accuracy: 0.967\n",
      "Validation accuracy: 0.968\n",
      "Validation accuracy: 0.971\n",
      "Validation accuracy: 0.966\n",
      "w_1 norm 55.813805\n",
      "w_2 norm 37.798588\n",
      "Accuracy on Current Dataset 0.9721\n",
      "Test on all Previous Datasets:\n",
      "0.9632\n",
      "0.9714\n",
      "0.9712\n",
      "0.9721\n",
      "Current Average Accuracy: 0.9694750000000001\n",
      "Training MNIST 5\n",
      "Validation accuracy: 0.14\n",
      "Validation accuracy: 0.945\n",
      "Validation accuracy: 0.952\n",
      "Validation accuracy: 0.962\n",
      "Validation accuracy: 0.959\n",
      "Validation accuracy: 0.963\n",
      "Validation accuracy: 0.964\n",
      "Validation accuracy: 0.963\n",
      "Validation accuracy: 0.963\n",
      "Validation accuracy: 0.963\n",
      "w_1 norm 60.901585\n",
      "w_2 norm 39.996304\n",
      "Accuracy on Current Dataset 0.9718\n",
      "Test on all Previous Datasets:\n",
      "0.9621\n",
      "0.9705\n",
      "0.9698\n",
      "0.9697\n",
      "0.9718\n",
      "Current Average Accuracy: 0.96878\n",
      "Training MNIST 6\n",
      "Validation accuracy: 0.16\n",
      "Validation accuracy: 0.935\n",
      "Validation accuracy: 0.956\n",
      "Validation accuracy: 0.957\n",
      "Validation accuracy: 0.955\n",
      "Validation accuracy: 0.957\n",
      "Validation accuracy: 0.955\n",
      "Validation accuracy: 0.957\n",
      "Validation accuracy: 0.958\n",
      "Validation accuracy: 0.964\n",
      "w_1 norm 65.24078\n",
      "w_2 norm 41.757248\n",
      "Accuracy on Current Dataset 0.9681\n",
      "Test on all Previous Datasets:\n",
      "0.958\n",
      "0.9675\n",
      "0.9663\n",
      "0.9692\n",
      "0.9678\n",
      "0.9681\n",
      "Current Average Accuracy: 0.9661499999999998\n",
      "Training MNIST 7\n",
      "Validation accuracy: 0.073\n",
      "Validation accuracy: 0.936\n",
      "Validation accuracy: 0.951\n",
      "Validation accuracy: 0.948\n",
      "Validation accuracy: 0.951\n",
      "Validation accuracy: 0.96\n",
      "Validation accuracy: 0.954\n",
      "Validation accuracy: 0.958\n",
      "Validation accuracy: 0.958\n",
      "Validation accuracy: 0.953\n",
      "w_1 norm 69.00595\n",
      "w_2 norm 43.088005\n",
      "Accuracy on Current Dataset 0.9672\n",
      "Test on all Previous Datasets:\n",
      "0.9538\n",
      "0.9615\n",
      "0.9639\n",
      "0.9662\n",
      "0.9654\n",
      "0.9655\n",
      "0.9672\n",
      "Current Average Accuracy: 0.9633571428571427\n",
      "Training MNIST 8\n",
      "Validation accuracy: 0.139\n",
      "Validation accuracy: 0.939\n",
      "Validation accuracy: 0.951\n",
      "Validation accuracy: 0.95\n",
      "Validation accuracy: 0.954\n",
      "Validation accuracy: 0.95\n",
      "Validation accuracy: 0.954\n",
      "Validation accuracy: 0.955\n",
      "Validation accuracy: 0.957\n",
      "Validation accuracy: 0.959\n",
      "w_1 norm 72.10861\n",
      "w_2 norm 44.186394\n",
      "Accuracy on Current Dataset 0.9638\n",
      "Test on all Previous Datasets:\n",
      "0.9477\n",
      "0.9604\n",
      "0.9633\n",
      "0.9623\n",
      "0.9628\n",
      "0.9627\n",
      "0.964\n",
      "0.9638\n",
      "Current Average Accuracy: 0.960875\n",
      "Training MNIST 9\n",
      "Validation accuracy: 0.131\n",
      "Validation accuracy: 0.922\n",
      "Validation accuracy: 0.944\n",
      "Validation accuracy: 0.951\n",
      "Validation accuracy: 0.95\n",
      "Validation accuracy: 0.953\n",
      "Validation accuracy: 0.95\n",
      "Validation accuracy: 0.951\n",
      "Validation accuracy: 0.956\n",
      "Validation accuracy: 0.959\n",
      "w_1 norm 74.72542\n",
      "w_2 norm 45.1285\n",
      "Accuracy on Current Dataset 0.9629\n",
      "Test on all Previous Datasets:\n",
      "0.9463\n",
      "0.9567\n",
      "0.9566\n",
      "0.9605\n",
      "0.9593\n",
      "0.9583\n",
      "0.962\n",
      "0.962\n",
      "0.9629\n",
      "Current Average Accuracy: 0.9582888888888887\n",
      "Training MNIST 10\n",
      "Validation accuracy: 0.105\n",
      "Validation accuracy: 0.935\n",
      "Validation accuracy: 0.942\n",
      "Validation accuracy: 0.96\n",
      "Validation accuracy: 0.96\n",
      "Validation accuracy: 0.959\n",
      "Validation accuracy: 0.964\n",
      "Validation accuracy: 0.963\n",
      "Validation accuracy: 0.964\n",
      "Validation accuracy: 0.962\n",
      "w_1 norm 76.91584\n",
      "w_2 norm 45.994816\n",
      "Accuracy on Current Dataset 0.9609\n",
      "Test on all Previous Datasets:\n",
      "0.9391\n",
      "0.952\n",
      "0.9527\n",
      "0.9553\n",
      "0.9542\n",
      "0.9542\n",
      "0.9571\n",
      "0.9583\n",
      "0.9607\n",
      "0.9609\n",
      "Current Average Accuracy: 0.9544500000000001\n"
     ]
    }
   ],
   "source": [
    "#Training the network sequentially on the 10 datasets\n",
    "task_num = 10\n",
    "avg_list = []\n",
    "SA0_list = []\n",
    "SA1_list = []\n",
    "prev_list = [[] for x in xrange(task_num)]\n",
    "     \n",
    "\n",
    "#Iterate over 10 tasks\n",
    "for j in xrange(task_num):\n",
    "    print \"Training MNIST %d\" % (j+1)\n",
    "    \n",
    "    #Update the parameters for 10000 times\n",
    "    for i in xrange(10000):\n",
    "        batch_xs, batch_ys = mnist_list[j].train.next_batch(50)\n",
    "\n",
    "        sess.run(step, feed_dict = {a_0: batch_xs, y: batch_ys})\n",
    "        \n",
    "        #Print validation accuracy every 1000 steps\n",
    "        if i % 1000 == 0:\n",
    "            res = sess.run(acct_res, feed_dict =\n",
    "                           {a_0:mnist_list[j].validation.images[:1000],\n",
    "                            y : mnist_list[j].validation.labels[:1000]})\n",
    "            print \"Validation accuracy:\", res/1000\n",
    "\n",
    "    sess.run(updateW_old)\n",
    "    \n",
    "    print \"w_1 norm\", sess.run(tf.reduce_sum(tf.norm(w_1)))\n",
    "    print \"w_2 norm\", sess.run(tf.reduce_sum(tf.norm(w_2)))\n",
    "    \n",
    "    #Print the accuracies on testing set of the task just trained on\n",
    "    res = sess.run(acct_res, feed_dict =\n",
    "                           {a_0: mnist_list[j].test.images[:100000],\n",
    "                            y : mnist_list[j].test.labels[:100000]})\n",
    "    print \"Accuracy on Current Dataset\", res/mnist.test.labels.shape[0]\n",
    "    \n",
    "    res_sum = 0\n",
    "    \n",
    "    print \"Test on all Previous Datasets:\" \n",
    "    for i in xrange(j+1):\n",
    "        res = sess.run(acct_res, feed_dict =\n",
    "                               {a_0: mnist_list[i].test.images[:100000],\n",
    "                                y : mnist_list[i].test.labels[:100000]})\n",
    "        acc_res = res/mnist.test.labels.shape[0]\n",
    "        print acc_res\n",
    "        prev_list[i].append(acc_res)\n",
    "        res_sum += acc_res\n",
    "        avg_res = res_sum/(j+1)\n",
    "    print \"Current Average Accuracy:\", avg_res\n",
    "\n",
    "    avg_list.append(avg_res)\n",
    "    \n",
    "    #Collecting activation vectors to compute conceptors on the input layer\n",
    "    batch_xs, batch_ys = mnist_list[j].train.next_batch(500)\n",
    "    a0_collection = batch_xs\n",
    "    ab0_collection = sess.run(ab_0, feed_dict = {a_0: a0_collection})\n",
    "\n",
    "    alpha = 4\n",
    "    R_a0 = (ab0_collection.T).dot(ab0_collection) / ab0_collection.shape[0]\n",
    "    U_a0, S_a0, _ = np.linalg.svd(R_a0)\n",
    "    S_C0 = (np.diag(S_a0).dot(np.linalg.inv(np.diag(S_a0) + alpha ** (-2) * np.eye(ab0_collection.shape[1]))))\n",
    "    S0 = np.diag(S_C0)    \n",
    "    C0 = U_a0.dot(np.diag(S0)).dot(U_a0.T)\n",
    "\n",
    "    #Collecting activation vectors to compute conceptors on the hidden layer\n",
    "    ab1_collection = sess.run(ab_1, feed_dict = {a_0: a0_collection})\n",
    "    \n",
    "    alpha1 = 4\n",
    "    R_a1 = (ab1_collection.T).dot(ab1_collection) / ab1_collection.shape[0]\n",
    "    U_a1, S_a1, _ = np.linalg.svd(R_a1)\n",
    "    S_C1 = (np.diag(S_a1).dot(np.linalg.inv(np.diag(S_a1) + alpha1 ** (-2) * np.eye(ab1_collection.shape[1]))))\n",
    "    S1 = np.diag(S_C1)\n",
    "    C1 = U_a1.dot(np.diag(S1)).dot(U_a1.T)\n",
    "\n",
    "    #Update the conceptors for used spaces on each layer\n",
    "    A_0 = logic.OR(C0, A_0)\n",
    "    A_1 = logic.OR(C1, A_1)\n",
    "\n",
    "    #Update the conceptors for free space on each layer\n",
    "    F0 = logic.NOT(A_0)\n",
    "    F1 = logic.NOT(A_1)\n",
    "    \n",
    "    updateF = [tf.assign(F_0, tf.cast(F0, tf.float32)), tf.assign(F_1, tf.cast(F1, tf.float32))]\n",
    "    sess.run(updateF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
